import psycopg2
import pandas as pd
import boto3
from psycopg2 import sql

def get_user_inputs():
    """
    Prompt the user for Redshift and S3 details.
    """
    host = input("Enter Redshift cluster endpoint (e.g., redshift-cluster-1.xxxx.us-west-2.redshift.amazonaws.com): ")
    port = input("Enter Redshift port (default: 5439): ") or "5439"
    database = input("Enter Redshift database name: ")
    s3_path = input("Enter S3 file path (e.g., s3://your-bucket-name/your-file.csv): ")
    iam_role = input("Enter IAM Role ARN for Redshift (e.g., arn:aws:iam::account-id:role/your-role): ")
    table_name = input("Enter the name of the Redshift table to create: ")
    schema_name = input("Enter the schema name (default: public): ") or "public"
    local_csv_path = input("Enter the local path of the CSV for schema inference: ")
    region = input("Enter the AWS region of your S3 bucket and Redshift cluster (e.g., us-west-2): ")
    db_user = input("Enter a database user name (this can be any string, e.g., 'iam_user'): ")
    cluster_identifier = input("Enter your Redshift cluster identifier: ")

    return {
        "host": host,
        "port": port,
        "database": database,
        "s3_path": s3_path,
        "iam_role": iam_role,
        "table_name": table_name,
        "schema_name": schema_name,
        "local_csv_path": local_csv_path,
        "region": region,
        "db_user": db_user,
        "cluster_identifier": cluster_identifier
    }

def get_temporary_credentials(db_user, cluster_identifier, database, region):
    """
    Generate temporary database credentials using IAM role.
    """
    client = boto3.client('redshift', region_name=region)
    try:
        response = client.get_cluster_credentials(
            DbUser=db_user,
            DbName=database,
            ClusterIdentifier=cluster_identifier,
            AutoCreate=True,  # Set to True to auto-create the user if it doesn't exist
            DurationSeconds=3600  # Credentials valid for 1 hour
        )
        temp_username = response['DbUser']
        temp_password = response['DbPassword']
        return temp_username, temp_password
    except Exception as e:
        print(f"Failed to get temporary credentials: {e}")
        raise

def map_data_types(df):
    """
    Map pandas dtypes to Redshift SQL data types.
    """
    dtype_mapping = {
        'object': 'VARCHAR(65535)',
        'int64': 'BIGINT',
        'int32': 'INTEGER',
        'float64': 'DOUBLE PRECISION',
        'float32': 'REAL',
        'bool': 'BOOLEAN',
        'datetime64[ns]': 'TIMESTAMP',
    }
    columns = []
    for col in df.columns:
        dtype = str(df[col].dtype)
        col_type = dtype_mapping.get(dtype, 'VARCHAR(65535)')
        columns.append((col, col_type))
    return columns

def quote_identifier(identifier):
    """
    Safely quote SQL identifiers to handle special characters and reserved words.
    """
    return sql.Identifier(identifier)

def create_table_from_csv(csv_file_path, table_name, schema_name, conn):
    """
    Create a table in Redshift by inferring schema from the CSV file structure.
    """
    # Load the CSV locally to infer schema
    df = pd.read_csv(csv_file_path, nrows=1000)  # Read a sample to infer schema

    # Infer column names and types
    columns = map_data_types(df)

    # Construct the CREATE TABLE statement
    column_defs = [
        sql.SQL("{} {}").format(quote_identifier(col_name), sql.SQL(col_type))
        for col_name, col_type in columns
    ]
    create_table_query = sql.SQL("""
        CREATE TABLE IF NOT EXISTS {}.{} (
            {}
        );
    """).format(
        quote_identifier(schema_name),
        quote_identifier(table_name),
        sql.SQL(', ').join(column_defs)
    )

    print("Executing CREATE TABLE query...")
    try:
        with conn.cursor() as cur:
            cur.execute(create_table_query)
            conn.commit()
        print(f"Table {schema_name}.{table_name} created successfully.")
    except Exception as e:
        conn.rollback()
        print(f"Failed to create table: {e}")
        raise

def load_data_to_redshift(schema_name, table_name, s3_path, iam_role, region, conn):
    """
    Load data from S3 into the Redshift table using the COPY command.
    """
    copy_query = sql.SQL("""
        COPY {}.{}
        FROM %s
        IAM_ROLE %s
        REGION %s
        DELIMITER ','
        IGNOREHEADER 1
        FORMAT AS CSV
        EMPTYASNULL
        BLANKSASNULL
        TIMEFORMAT 'auto'
        TRUNCATECOLUMNS
        ACCEPTINVCHARS
        COMPUPDATE OFF
        STATUPDATE OFF;
    """).format(
        quote_identifier(schema_name),
        quote_identifier(table_name)
    )

    print("Executing COPY command...")
    try:
        with conn.cursor() as cur:
            cur.execute(copy_query, (s3_path, iam_role, region))
            conn.commit()
        print(f"Data loaded into table {schema_name}.{table_name} successfully.")
    except Exception as e:
        conn.rollback()
        print(f"Failed to load data: {e}")
        raise

def main():
    """
    Main function to prompt user inputs, create a table, and load data into Redshift.
    """
    conn = None
    try:
        # Prompt for inputs
        inputs = get_user_inputs()

        # Get temporary credentials
        temp_username, temp_password = get_temporary_credentials(
            db_user=inputs["db_user"],
            cluster_identifier=inputs["cluster_identifier"],
            database=inputs["database"],
            region=inputs["region"]
        )

        # Connect to Redshift using temporary credentials
        conn = psycopg2.connect(
            host=inputs["host"],
            port=inputs["port"],
            database=inputs["database"],
            user=temp_username,
            password=temp_password
        )
        print("Connected to Redshift using IAM authentication")

        # Step 1: Create the table
        print("Inferring schema and creating table...")
        create_table_from_csv(
            inputs["local_csv_path"],
            inputs["table_name"],
            inputs["schema_name"],
            conn
        )

        # Step 2: Load the data
        print("Loading data into Redshift...")
        load_data_to_redshift(
            inputs["schema_name"],
            inputs["table_name"],
            inputs["s3_path"],
            inputs["iam_role"],
            inputs["region"],
            conn
        )

    except Exception as e:
        print(f"Error: {e}")

    finally:
        if conn:
            conn.close()

if __name__ == "__main__":
    main()
